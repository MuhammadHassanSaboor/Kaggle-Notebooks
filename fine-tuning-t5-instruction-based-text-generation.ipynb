{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üöÄ Fine-Tuning T5 for Instruction-Based Text Generation üß†\n\nWelcome to this notebook where we explore the exciting world of **instruction-based fine-tuning** using the powerful **T5 Transformer model** from Hugging Face! ü§ó‚ú®\n\n---\n\n## üìå Objective\n\nThe goal of this notebook is to fine-tune the lightweight `t5-small` model on a contextual dataset of instructions, inputs, and expected outputs. This process, known as **Supervised Fine-Tuning (SFT)**, teaches the model to generate accurate and contextually relevant outputs based on structured instructions.\n\n---\n\n## üìä Dataset Overview\n\nWe use a rich dataset containing:\n\n- **Instruction** üìù ‚Äì The task description\n- **Input** üî° ‚Äì Context or additional information\n- **Output** üí¨ ‚Äì Desired response from the model\n- **Domain** üåê ‚Äì Task domain (e.g., NLP, Coding, QA)\n- **Source** üìÅ ‚Äì Origin of the instruction\n- **Quality Score** ‚≠ê ‚Äì Human-annotated score for output quality\n\nWe visualize the dataset with insightful charts to better understand the data distribution before training.\n\n---\n\n## üß™ Workflow Summary\n\nHere's what we cover in this notebook:\n\n1. üîç **Exploratory Data Analysis (EDA)** ‚Äì Understanding domain distribution, output length, and instruction trends.\n2. üßπ **Data Preprocessing** ‚Äì Tokenizing text with T5 tokenizer and preparing inputs/labels.\n3. üèãÔ∏è **Model Fine-Tuning** ‚Äì Training the `t5-small` model using Hugging Face's `Trainer`.\n4. üìâ **Loss Visualization** ‚Äì Plotting training loss over time.\n5. üß™ **Testing** ‚Äì Running test prompts to see the model‚Äôs predictions!\n\n---\n\n## üõ†Ô∏è Tools & Libraries Used\n\n- ü§ó **Transformers** (T5, Trainer)\n- üìä **Matplotlib** & **Seaborn** (visualizations)\n- üßº **Regex** & **WordCloud** (text cleaning + word cloud)\n- üêç **PyTorch** (dataset creation)\n- üìÅ **Pandas** (data handling)\n\n---\n\nLet's dive in and teach our model how to understand and complete instructions like a pro! üòéüî•\n","metadata":{}},{"cell_type":"markdown","source":"# üõ†Ô∏è **Installing Modules**","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install -q transformers datasets\n# !pip install sentencepiece\n# !pip install hf_xet\n# !pip install accelerate>=0.26.0\n# !pip install wordcloud\n# !pip install openpyxl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìö **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport re\nfrom wordcloud import WordCloud\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments , AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ‚öôÔ∏è **Basic Important Settings**","metadata":{}},{"cell_type":"code","source":"plt.style.use('dark_background')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìÇ **Loading Dataset**","metadata":{}},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/contextual-input-sft-dataset/SFT_Contextual_10000.xlsx', sheet_name='Sheet1')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç **Exploring Dataset**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sample(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä **Exploratory Data Analysis: Basic (EDA)**","metadata":{}},{"cell_type":"markdown","source":"## **üìä Domain-Level Analysis**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(data=df, y='domain', order=df['domain'].value_counts().index)\nplt.title('Instruction Counts by Domain')\nplt.xlabel('Number of Instructions')\nplt.ylabel('Domain')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üß† Quality Score Distribution**","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df, x='quality_score')\nplt.title('Distribution of Quality Scores')\nplt.xlabel('Quality Score')\nplt.ylabel('Count')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üßÆ Compare Human vs Synthetic**","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df, x='source', hue='quality_score')\nplt.title('Quality Scores by Source')\nplt.xlabel('Source')\nplt.ylabel('Count')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üß¨ Word Count Analysis (Instruction/Input/Output)**","metadata":{}},{"cell_type":"code","source":"df['instruction_length'] = df['instruction'].apply(lambda x: len(str(x).split()))\ndf['input_length'] = df['input'].apply(lambda x: len(str(x).split()))\ndf['output_length'] = df['output'].apply(lambda x: len(str(x).split()))\n\ndf[['instruction_length', 'input_length', 'output_length']].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=df[['instruction_length', 'input_length', 'output_length']])\nplt.title('Word Count Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üìà Quality Score vs Output Length**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=df, x='quality_score', y='output_length')\nplt.title('Output Length by Quality Score')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    return re.sub(r'[^\\w\\s]', '', text.lower())\n\nall_text = \" \".join(df['instruction'].apply(clean_text))\n\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Most Common Words in Instructions\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['quality_score'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['source'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['output'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['domain'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üéØ **Fine Tuning**","metadata":{}},{"cell_type":"markdown","source":"## **Prepare Data (10k ‚Üí 2k for speed)**","metadata":{}},{"cell_type":"code","source":"small_df = df.sample(2000, random_state=42)\ntrain_df, val_df = train_test_split(small_df, test_size=0.1)\n\ntrain_texts = [\"instruction: \" + i + \" input: \" + inp for i, inp in zip(train_df['instruction'], train_df['input'])]\nval_texts = [\"instruction: \" + i + \" input: \" + inp for i, inp in zip(val_df['instruction'], val_df['input'])]\n\ntrain_labels = train_df['output'].tolist()\nval_labels = val_df['output'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Tokenization**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\ndef preprocess(texts, labels):\n    model_inputs = tokenizer(texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n    labels = tokenizer(labels, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100  # mask loss\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntrain_encodings = preprocess(train_texts, train_labels)\nval_encodings = preprocess(val_texts, val_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Dataset Object**","metadata":{}},{"cell_type":"code","source":"class SFTDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n    def __getitem__(self, idx):\n        return {k: v[idx] for k, v in self.encodings.items()}\n\ntrain_dataset = SFTDataset(train_encodings)\nval_dataset = SFTDataset(val_encodings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Define and Train Model**","metadata":{}},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=1,\n    fp16=False  # turn off if no GPU\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä **Visualizing Results**","metadata":{}},{"cell_type":"code","source":"loss_values = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\nsteps = list(range(1, len(loss_values) + 1))\n\nplt.figure(figsize=(10, 5))\nplt.plot(steps, loss_values, label=\"Training Loss\")\nplt.xlabel(\"Logging Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß™ **Test Your Fine-Tuned Model**","metadata":{}},{"cell_type":"code","source":"def test_model(input_text):\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n    output_ids = model.generate(input_ids, max_length=100)\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\"Translate to German: I love data science.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\"Translate to French: I love data science.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(\"Translate to Romanian: I love data science.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ThankYou**","metadata":{}}]}